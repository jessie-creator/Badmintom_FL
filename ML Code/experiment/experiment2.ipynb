{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["hgbgBJFzGDel","CjWDe9VlGGZ6","XUXB82r9n6Ej","DdHy_UaToMuU","VnQqx6iMGkQp","crNSz29KGihe","ilFAL8-vG-8G","JFjhAarDHBfS","WLxK7DyyKXIL","timiF9diKcJ3","X7G0CXA_G64h","eUqbtd1tHNQz","2NlyR_PwG9CM","-dDwiU-_HWQX","i1nRTEwyHwzB","MTBHraUEH9tx","ted_VY9zJxl1","zORDjukdPjM9","lfdJFO5BYSCy","U_GGOR66aDxG","xtp_lLKn6970","roRab0NEYSCz","W4ElM_xRedTI","ZC0QXP_oGBlN"],"authorship_tag":"ABX9TyM9ZDtMr5RYCm6B5qfndDx1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# i=[]\n","# while(True):\n","#   i.append(\"a\")"],"metadata":{"id":"At7gxxf0UU0r"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q9lQd_gwiQ-L"},"outputs":[],"source":["import numpy as np\n","import os\n","import csv\n","import random\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import LSTM, GRU, Dense, Input, Dropout"]},{"cell_type":"code","source":["# set manual seed for reproducibility\n","seed = 33\n","\n","# general reproducibility\n","random.seed(seed)\n","np.random.seed(seed)"],"metadata":{"id":"_zI3Au8NF9e6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#constant\n","num_lable_kinds = 5\n","both_cols = 6\n","high_rows = 60\n","short_rows = 50\n","\n","output_dim = 5"],"metadata":{"id":"UpOMB36gGF3P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#numpy to store data\n","high_data = np.empty((0, high_rows, both_cols), dtype=float)\n","high_label = np.empty((0, num_lable_kinds), dtype=int)\n","short_data = np.empty((0, short_rows, both_cols), dtype=float)\n","short_label = np.empty((0, num_lable_kinds), dtype=int)"],"metadata":{"id":"X3bmdEwIGN7g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# predefine"],"metadata":{"id":"hgbgBJFzGDel"}},{"cell_type":"markdown","source":["## function of preprocess"],"metadata":{"id":"CjWDe9VlGGZ6"}},{"cell_type":"markdown","source":["### function of finding a hit"],"metadata":{"id":"XUXB82r9n6Ej"}},{"cell_type":"code","source":["def slope(input_data):\n","    slope_col = np.zeros(len(input_data),dtype=float)\n","    for i in range(len(input_data) - 1):\n","        for j in range(1,4):\n","            temp = input_data[i+1][j] - input_data[i][j]\n","            slope_col[i] += temp**2\n","        slope_col[i] = slope_col[i]**0.5\n","    return slope_col\n","\n","def ma(slope_col, n):\n","    slope_ma = np.zeros(len(slope_col),dtype=float)\n","    for i in range(n,len(slope_col)-n):\n","        for j in range(-n,n+1):\n","            slope_ma[i] += slope_col[i+j]\n","        slope_ma[i] /= float(2*n+1)\n","    return slope_ma\n","\n","def data_cut(input_data, save_data, hit_type):\n","    slope_col = slope(input_data)\n","    slope_avg = np.average(slope_col)\n","    if hit_type == \"high\":\n","        n = 6\n","    elif hit_type == \"short\":\n","        n = 5\n","    slope_ma = ma(slope_col, n)\n","\n","    for i in range(60,len(slope_col)-50):\n","\n","        if ( slope_ma[i] > slope_avg ) and ( slope_col[i]==max(slope_col[i-50:i+50]) ) : # 找到可能峰值\n","            start = 0  # 向前&向後找起點\n","            end = 0\n","            while i+start > (50+n) :\n","                start -= 1\n","                if slope_ma[i+start] <= slope_avg:\n","                    break\n","            while i+end < (len(slope_col)-50-n) :\n","                end += 1\n","                if slope_ma[i+end] <= slope_avg:\n","                    break\n","\n","            if  hit_type == \"high\" and end-start > 40 and slope_ma[i] > 15: # 長遠球\n","                save_data.append(input_data[i-45:i+15, [1,2,3,5,6,7]])\n","\n","            elif hit_type == \"short\" and end-start > 20 and slope_ma[i] > 15: # 挑球\n","                save_data.append(input_data[i-35:i+15, [1,2,3,5,6,7]])"],"metadata":{"id":"THRm6y7RoAHK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### other function"],"metadata":{"id":"DdHy_UaToMuU"}},{"cell_type":"code","source":["# Removes null bytes from the input file and returns a sanitized version of the file.\n","def sanitize_file(input_file_path):\n","  sanitized_content = \"\"\n","  with open(input_file_path, 'r', encoding='utf-8', errors='replace') as f:\n","    content = f.read()\n","    sanitized_content = content.replace('\\x00', '')\n","\n","  return sanitized_content"],"metadata":{"id":"tk4e2AaooO4S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hit_preprocess(input_file_path, hit_type):\n","\n","  # Sanitize the file by removing null bytes\n","  sanitized_content = sanitize_file(input_file_path)\n","  lines = sanitized_content.split('\\n')\n","\n","  # Starting from the last line, move upwards until a complete line (with 7 commas) is found\n","  while lines and lines[-1].count(\",\") != 7:\n","    lines = lines[:-1]\n","\n","  # Load the (potentially modified) data into a numpy array\n","  input_data = np.loadtxt(lines, delimiter=\",\", dtype=float)\n","\n","  cut_data = []\n","  data_cut(input_data, cut_data, hit_type)\n","  cut_data = np.array(cut_data).astype(float)\n","\n","  datas_label = np.empty((0, num_lable_kinds), dtype=int)\n","  label_values = np.array([int(ch) for ch in input_filename[6:11]])  # 轉換為整數陣列\n","  label_values = label_values[np.newaxis, :]  # 增加一個維度以使其成為二維陣列\n","  for i in range(len(cut_data)):\n","    datas_label = np.vstack((datas_label, label_values))\n","\n","  return cut_data,datas_label"],"metadata":{"id":"hFl_natBoRvq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## preprocess"],"metadata":{"id":"VnQqx6iMGkQp"}},{"cell_type":"code","source":["# 指定資料集所在的路徑\n","data_path = \"/content/\"\n","# 取得該路徑下所有的檔案名稱\n","all_files = os.listdir(data_path)\n","# 過濾出所有的 .txt 檔案\n","txt_files = [file_name for file_name in all_files if file_name.endswith('.txt')]\n","\n","for input_filename in txt_files:\n","\n","  if \"high\" in input_filename.lower():\n","    hit_type = \"high\"\n","  elif \"short\" in input_filename.lower():\n","    hit_type = \"short\"\n","  else:\n","    print(\"error txt-file\")\n","    continue\n","\n","  input_file_path = os.path.join(data_path, input_filename)\n","  data, label = hit_preprocess(input_file_path, hit_type)\n","\n","  if hit_type == \"high\":\n","    high_data = np.concatenate((high_data, data), axis=0)\n","    high_label = np.concatenate((high_label, label))\n","  elif hit_type == \"short\":\n","    short_data = np.concatenate((short_data, data), axis=0)\n","    short_label = np.concatenate((short_label, label))\n","\n","  os.remove(input_file_path)"],"metadata":{"id":"7bUhANjAGnVF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## function to reorder and split"],"metadata":{"id":"bKIKtYtMGdC9"}},{"cell_type":"markdown","source":["### shuffle, shuffle_direct"],"metadata":{"id":"crNSz29KGihe"}},{"cell_type":"code","source":["#input:data and label, output:numpy with same shape, but shuffled\n","#process: shuffle (data,label)\n","def shuffle(datas,labels):\n","  if len(datas) != len(labels):\n","    print(\"error\")\n","    return\n","\n","\n","  shuffled_datas = []\n","  shuffled_labels = []\n","\n","  # Shuffle the data indices\n","  indices = np.arange(len(datas))\n","  np.random.shuffle(indices)\n","\n","  for i in range(len(datas)):\n","    shuffled_datas.append(datas[indices[i]])\n","    shuffled_labels.append(labels[indices[i]])\n","\n","  return np.array(shuffled_datas), np.array(shuffled_labels)"],"metadata":{"id":"1CFXbX6eGhFi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#direct change input's data and label\n","def shuffle_direct(datas,labels):\n","  if len(datas) != len(labels):\n","    print(\"error\")\n","    return\n","\n","\n","  shuffled_datas_list = []\n","  shuffled_labels_list = []\n","\n","  # Shuffle the data indices\n","  indices = np.arange(len(datas))\n","  np.random.shuffle(indices)\n","\n","  for i in range(len(datas)):\n","    shuffled_datas_list.append(datas[indices[i]])\n","    shuffled_labels_list.append(labels[indices[i]])\n","\n","  shuffled_datas = np.array(shuffled_datas_list)\n","  shuffled_labels = np.array(shuffled_labels_list)\n","\n","  datas[:] = shuffled_datas\n","  labels[:] = shuffled_labels"],"metadata":{"id":"GVtdS9jHGnEy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### sort, sort_direct"],"metadata":{"id":"ilFAL8-vG-8G"}},{"cell_type":"code","source":["#input:data and label, output:numpy with same shape, but sorted\n","#process: sort (data,label)\n","def sort(datas,labels):\n","  if len(datas) != len(labels):\n","    print(\"error\")\n","    return\n","\n","  label_sums = labels.sum(axis=1)\n","  sorted_indice = np.argsort(label_sums,kind='stable')\n","\n","  sorted_datas = []\n","  sorted_labels = []\n","\n","  for i in range(len(datas)):\n","    sorted_datas.append(datas[sorted_indice[i]])\n","    sorted_labels.append(labels[sorted_indice[i]])\n","\n","  return np.array(sorted_datas), np.array(sorted_labels)"],"metadata":{"id":"XlspZ7cUGpC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#direct change input's data and label\n","def sort_direct(datas,labels):\n","  if len(datas) != len(labels):\n","    print(\"error\")\n","    return\n","\n","  label_sums = labels.sum(axis=1)\n","  sorted_indice = np.argsort(label_sums,kind='stable')\n","\n","  sorted_datas_list = []\n","  sorted_labels_list = []\n","\n","  for i in range(len(datas)):\n","    sorted_datas_list.append(datas[sorted_indice[i]])\n","    sorted_labels_list.append(labels[sorted_indice[i]])\n","\n","  sorted_datas = np.array(sorted_datas_list)\n","  sorted_labels = np.array(sorted_labels_list)\n","\n","  datas[:] = sorted_datas\n","  labels[:] = sorted_labels"],"metadata":{"id":"tvdxpUZCGtUD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### split_percentage, split_equal_to_n"],"metadata":{"id":"JFjhAarDHBfS"}},{"cell_type":"code","source":["#input: data, label and percentage of test\n","#output: numpy, (train_data, train_label, test_data, test_label)\n","def split_percentage(datas,labels,test_percent):\n","  num_train = round(len(datas) * (1 - test_percent))\n","\n","  train_data = datas[:num_train]\n","  train_label = labels[:num_train]\n","  test_data = datas[num_train:]\n","  test_label = labels[num_train:]\n","  return train_data, train_label, test_data, test_label"],"metadata":{"id":"z6Dbbk-7HDnE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#input: data, label and number of client\n","#output: numpy, [ data1 data2 ...], data1 = output[0]\n","def split_equal_to_n(datas,labels,n_Clients):\n","  num_items_per_client = len(datas) // n_Clients\n","  client_data = []\n","  client_label = []\n","\n","  for i in range(n_Clients):\n","    start_idx = i * num_items_per_client\n","    end_idx = (i + 1) * num_items_per_client\n","    client_data.append(datas[start_idx:end_idx])\n","    client_label.append(labels[start_idx:end_idx])\n","\n","  return np.array(client_data), np.array(client_label)"],"metadata":{"id":"0bDMkr-KHGFe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## function to average weight and split score"],"metadata":{"id":"6BIx5XlgKJ8p"}},{"cell_type":"markdown","source":["### average_weight"],"metadata":{"id":"WLxK7DyyKXIL"}},{"cell_type":"code","source":["def average_weight(models):\n","  weights = [model.get_weights() for model in models]\n","\n","  avg_weights = list()\n","  for weights_list_tuple in zip(*weights):\n","    avg_weights.append(\n","        np.array([np.array(w).mean(axis=0) for w in zip(*weights_list_tuple)])\n","    )\n","\n","  return avg_weights"],"metadata":{"id":"GCmX0gkGKaFC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### split_label_score"],"metadata":{"id":"timiF9diKcJ3"}},{"cell_type":"code","source":["def split_label_score(datas,labels):\n","  if len(datas) != len(labels):\n","    print(\"error\")\n","    return\n","\n","  client_data = []\n","  client_label = []\n","\n","  stdt, stlb = sort(datas,labels)\n","  means = np.mean(stlb, axis=1)\n","  rounded_means = np.round(means)\n","\n","  start_idx = 0\n","  end_idx = 0\n","  temp_label = 1\n","\n","  for i in range(len(stlb)):\n","    if rounded_means[i] != temp_label:\n","      end_idx = i\n","      client_data.append(stdt[start_idx:end_idx])\n","      client_label.append(stlb[start_idx:end_idx])\n","      start_idx = i\n","      temp_label = temp_label + 1\n","  end_idx = len(stlb)\n","  client_data.append(stdt[start_idx:end_idx])\n","  client_label.append(stlb[start_idx:end_idx])\n","\n","  while temp_label < num_lable_kinds :\n","    client_data.append(stdt[end_idx:end_idx])\n","    client_label.append(stlb[end_idx:end_idx])\n","    temp_label=temp_label+1\n","\n","  return np.array(client_data), np.array(client_label)"],"metadata":{"id":"Mu78bw43KfnY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# models"],"metadata":{"id":"X7G0CXA_G64h"}},{"cell_type":"markdown","source":["### output, loss, accuracy"],"metadata":{"id":"eUqbtd1tHNQz"}},{"cell_type":"code","source":["# Define custom output function\n","def custom_output(x):\n","  # Implement your custom output logic here\n","  return x\n","\n","def custom_loss(y_true, y_pred):\n","  y_true = tf.cast(y_true, dtype=tf.float32)  # Cast y_true to float32\n","  loss = tf.reduce_mean(tf.square(y_true - y_pred))\n","  return loss\n","\n","# Define custom accuracy function\n","threshold = 0.5\n","def custom_accuracy(y_true, y_pred):\n","  abs_diff = tf.abs(y_true - y_pred)\n","  condition = tf.less_equal(abs_diff, threshold)\n","  acc = tf.cast(condition, tf.float32)\n","  return acc"],"metadata":{"id":"HHWPVcKSHLEb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### LSTM_mymodel"],"metadata":{"id":"2NlyR_PwG9CM"}},{"cell_type":"code","source":["class LSTM_mymodel(Model):\n","    def __init__(self, units, output_dim, num_layers):\n","        super(LSTM_mymodel, self).__init__()\n","        self.lstm_layers = [LSTM(units, return_sequences=(i < num_layers - 1)) for i in range(num_layers)]\n","        self.dense = Dense(output_dim)\n","\n","    def call(self, inputs):\n","        x = inputs\n","        for lstm_layer in self.lstm_layers:\n","            x = lstm_layer(x)\n","        output = self.dense(x)\n","        output = custom_output(output)\n","        return output"],"metadata":{"id":"HMcC6_n9G6jD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### LSTMdrop_mymodel"],"metadata":{"id":"-dDwiU-_HWQX"}},{"cell_type":"code","source":["class LSTMdrop_mymodel(Model):\n","    def __init__(self, units, output_dim, num_layers, dropout_rate, learning_rate=0.01):\n","        super(LSTMdrop_mymodel, self).__init__()\n","        self.lstm_layers = [LSTM(units, return_sequences=(i < num_layers - 1)) for i in range(num_layers)]\n","        self.dropout = Dropout(rate=dropout_rate)\n","        self.dense = Dense(output_dim)\n","\n","    def call(self, inputs):\n","        x = inputs\n","        for lstm_layer in self.lstm_layers:\n","            x = lstm_layer(x)\n","            x = self.dropout(x)  # Apply dropout after each LSTM layer\n","        output = self.dense(x)\n","        output = custom_output(output)\n","        return output"],"metadata":{"id":"YOmFrmdwHfp_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GRU_mymodel"],"metadata":{"id":"i1nRTEwyHwzB"}},{"cell_type":"code","source":["class GRU_mymodel(Model):\n","    def __init__(self, units, output_dim, num_layers):\n","        super(GRU_mymodel, self).__init__()\n","        self.gru_layers = [GRU(units, return_sequences=(i < num_layers - 1)) for i in range(num_layers)]\n","        self.dense = Dense(output_dim)\n","\n","    def call(self, inputs):\n","        x = inputs\n","        for gru_layer in self.gru_layers:\n","            x = gru_layer(x)\n","        output = self.dense(x)\n","        output = custom_output(output)\n","        return output"],"metadata":{"id":"0bKyV00wHwdQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GRUdrop_mymodel"],"metadata":{"id":"MTBHraUEH9tx"}},{"cell_type":"code","source":["class GRUdrop_mymodel(Model):\n","    def __init__(self, units, output_dim, num_layers, dropout_rate):\n","        super(GRUdrop_mymodel, self).__init__()\n","        self.gru_layers = [GRU(units, return_sequences=(i < num_layers - 1)) for i in range(num_layers)]\n","        self.dropout = Dropout(rate=dropout_rate)\n","        self.dense = Dense(output_dim)\n","\n","    def call(self, inputs):\n","        x = inputs\n","        for gru_layer in self.gru_layers:\n","            x = gru_layer(x)\n","            x = self.dropout(x)  # Apply dropout after each GRU layer\n","        output = self.dense(x)\n","        output = custom_output(output)\n","        return output"],"metadata":{"id":"d23wImFQIEZF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# test function"],"metadata":{"id":"ted_VY9zJxl1"}},{"cell_type":"markdown","source":["### runiidLSTM"],"metadata":{"id":"zORDjukdPjM9"}},{"cell_type":"code","source":["def runiidLSTM(datas,labels,test_data,test_label,num_batch):\n","  iid_LSTM_models = [] #high: 長球\n","\n","  for i in range(num_model):\n","    iid_LSTM_models.append(LSTM_mymodel(units=64, output_dim=output_dim, num_layers=num_layers))\n","\n","  for model in iid_LSTM_models:\n","    model.compile(optimizer='adam', loss=custom_loss, metrics=[custom_accuracy])\n","\n","  for j in range(num_round):\n","    for i, model in enumerate(iid_LSTM_models):\n","      model.fit(datas[i+j*num_model], labels[i+j*num_model],epochs=Epochs, batch_size=num_batch, verbose=0)\n","\n","    avg_weight = average_weight(iid_LSTM_models)\n","\n","    for model in iid_LSTM_models:\n","      model.set_weights(avg_weight)\n","\n","  print(\"\\nLSTM\")\n","  iid_LSTM_loss, iid_LSTM_accuracy = iid_LSTM_models[0].evaluate(test_data, test_label)"],"metadata":{"id":"V0R4P-EePNxU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### runsortLSTM"],"metadata":{"id":"lfdJFO5BYSCy"}},{"cell_type":"code","source":["def runsortLSTM(datas,labels,test_data,test_label,num_batch):\n","  sort_LSTM_models = [] #high: 長球\n","\n","  for i in range(num_model):\n","    sort_LSTM_models.append(LSTM_mymodel(units=64, output_dim=output_dim, num_layers=num_layers))\n","\n","  for model in sort_LSTM_models:\n","    model.compile(optimizer='adam', loss=custom_loss, metrics=[custom_accuracy])\n","\n","  for j in range(num_round):\n","    for i, model in enumerate(sort_LSTM_models):\n","      model.fit(datas[i*num_round+j], labels[i*num_round+j],epochs=Epochs, batch_size=num_batch, verbose=0)\n","\n","    avg_weight = average_weight(sort_LSTM_models)\n","\n","    for model in sort_LSTM_models:\n","      model.set_weights(avg_weight)\n","\n","  print(\"\\nLSTM\")\n","  sort_LSTM_loss, sort_LSTM_accuracy = sort_LSTM_models[0].evaluate(test_data, test_label)"],"metadata":{"id":"K_VrPGnbYSCz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### runscoreLSTM"],"metadata":{"id":"U_GGOR66aDxG"}},{"cell_type":"code","source":["def runscoreLSTM(datas_12,labels_12,datas_345,labels_345,test_data,test_label,num_batch):\n","  score_LSTM_models = [] #high: 長球\n","\n","  for i in range(2):\n","    score_LSTM_models.append(LSTM_mymodel(units=64, output_dim=output_dim, num_layers=num_layers))\n","\n","  for model in score_LSTM_models:\n","    model.compile(optimizer='adam', loss=custom_loss, metrics=[custom_accuracy])\n","\n","  for j in range(num_round):\n","\n","    score_LSTM_models[0].fit(datas_12[j],labels_12[j],epochs=Epochs, batch_size=num_batch, verbose=0)\n","    score_LSTM_models[1].fit(datas_345[j],labels_345[j],epochs=Epochs, batch_size=num_batch, verbose=0)\n","\n","    avg_weight = average_weight(score_LSTM_models)\n","\n","    for model in score_LSTM_models:\n","      model.set_weights(avg_weight)\n","\n","  print(\"\\nLSTM\")\n","  score_LSTM_loss, score_LSTM_accuracy = score_LSTM_models[0].evaluate(test_data, test_label)"],"metadata":{"id":"rph7eixMaDxG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### runiidGRU"],"metadata":{"id":"xtp_lLKn6970"}},{"cell_type":"code","source":["def runiidGRU(datas,labels,test_data,test_label,num_batch):\n","  iid_GRU_models = [] #high: 長球\n","\n","  for i in range(num_model):\n","    iid_GRU_models.append(GRU_mymodel(units=64, output_dim=output_dim, num_layers=num_layers))\n","\n","  for model in iid_GRU_models:\n","    model.compile(optimizer='adam', loss=custom_loss, metrics=[custom_accuracy])\n","\n","  for j in range(num_round):\n","    for i, model in enumerate(iid_GRU_models):\n","      model.fit(datas[i+j*num_model], labels[i+j*num_model],epochs=Epochs, batch_size=num_batch, verbose=0)\n","\n","    avg_weight = average_weight(iid_GRU_models)\n","\n","    for model in iid_GRU_models:\n","      model.set_weights(avg_weight)\n","\n","  print(\"\\nGRU\")\n","  iid_GRU_loss, iid_GRU_accuracy = iid_GRU_models[0].evaluate(test_data, test_label)"],"metadata":{"id":"EsWkboVv697-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### runsortGRU"],"metadata":{"id":"roRab0NEYSCz"}},{"cell_type":"code","source":["def runsortGRU(datas,labels,test_data,test_label,num_batch):\n","  sort_GRU_models = [] #high: 長球\n","\n","  for i in range(num_model):\n","    sort_GRU_models.append(GRU_mymodel(units=64, output_dim=output_dim, num_layers=num_layers))\n","\n","  for model in sort_GRU_models:\n","    model.compile(optimizer='adam', loss=custom_loss, metrics=[custom_accuracy])\n","\n","  for j in range(num_round):\n","    for i, model in enumerate(sort_GRU_models):\n","      model.fit(datas[i*num_round+j], labels[i*num_round+j],epochs=Epochs, batch_size=num_batch, verbose=0)\n","\n","    avg_weight = average_weight(sort_GRU_models)\n","\n","    for model in sort_GRU_models:\n","      model.set_weights(avg_weight)\n","\n","  print(\"\\nGRU\")\n","  sort_GRU_loss, sort_GRU_accuracy = sort_GRU_models[0].evaluate(test_data, test_label)"],"metadata":{"id":"UOYnOSgCYSC0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### runscoreGRU"],"metadata":{"id":"W4ElM_xRedTI"}},{"cell_type":"code","source":["def runscoreGRU(datas_12,labels_12,datas_345,labels_345,test_data,test_label,num_batch):\n","  score_GRU_models = [] #high: 長球\n","\n","  for i in range(2):\n","    score_GRU_models.append(GRU_mymodel(units=64, output_dim=output_dim, num_layers=num_layers))\n","\n","  for model in score_GRU_models:\n","    model.compile(optimizer='adam', loss=custom_loss, metrics=[custom_accuracy])\n","\n","  for j in range(num_round):\n","\n","    score_GRU_models[0].fit(datas_12[j],labels_12[j],epochs=Epochs, batch_size=num_batch, verbose=0)\n","    score_GRU_models[1].fit(datas_345[j],labels_345[j],epochs=Epochs, batch_size=num_batch, verbose=0)\n","\n","    avg_weight = average_weight(score_GRU_models)\n","\n","    for model in score_GRU_models:\n","      model.set_weights(avg_weight)\n","\n","  print(\"\\nGRU\")\n","  score_GRU_loss, score_GRU_accuracy = score_GRU_models[0].evaluate(test_data, test_label)"],"metadata":{"id":"HIebfAtuedTI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# test"],"metadata":{"id":"Fj7k025teMvB"}},{"cell_type":"code","source":["# constant\n","test_percent = 0.1\n","num_layers = 1\n","Dropout_Rate = 0.2\n","\n","Epochs = 30\n","# Batch_Size = 32\n","# constant for fedavg\n","num_model = 3\n","num_round = 2\n","# constant\n","num_run = 3\n","\n","Batch = [10,16,20,24,32,40]\n"],"metadata":{"id":"fZDOLYlEeSgO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(high_data.shape)\n","shuffle_direct(high_data, high_label)\n","train_data, train_label, test_data, test_label = split_percentage(high_data,high_label,test_percent)\n","print(train_data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xG2yA1ICeZYM","executionInfo":{"status":"ok","timestamp":1700027619637,"user_tz":-480,"elapsed":54,"user":{"displayName":"專題羽球","userId":"16827757121216149927"}},"outputId":"b70c5611-ac40-49dc-e929-3b1e0ed50919"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(848, 60, 6)\n","(763, 60, 6)\n"]}]},{"cell_type":"markdown","source":["## Centralize"],"metadata":{"id":"jzHXVf5re5Sq"}},{"cell_type":"code","source":["for i in range(6):\n","\n","  Batch_Size = Batch[i]\n","  print(\"\\nbatchsize : \",Batch_Size)\n","\n","  for _ in range(num_run):\n","    cen_LSTM = LSTM_mymodel(units=64, output_dim=output_dim, num_layers=num_layers)\n","    cen_LSTM.compile(optimizer='adam', loss=custom_loss, metrics=[custom_accuracy])\n","    cen_LSTM.fit(train_data, train_label, epochs=Epochs, batch_size=Batch_Size, verbose=0)\n","    print(\"\\nLSTM\")\n","    cen_LSTM_loss, cen_LSTM_accuracy = cen_LSTM.evaluate(test_data, test_label)\n","\n","    cen_GRU = GRU_mymodel(units=64, output_dim=output_dim, num_layers=num_layers)\n","    cen_GRU.compile(optimizer='adam', loss=custom_loss, metrics=[custom_accuracy])\n","    cen_GRU.fit(train_data, train_label, epochs=Epochs, batch_size=Batch_Size, verbose=0)\n","    print(\"\\nGRU\")\n","    cen_GRU_loss, cen_GRU_accuracy = cen_GRU.evaluate(test_data, test_label)"],"metadata":{"id":"5B8lNC4se-Vv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## IID run"],"metadata":{"id":"PgWcoaJnT9TI"}},{"cell_type":"code","source":["# divide data\n","hi_iid_datas, hi_iid_labels = split_equal_to_n(train_data, train_label, num_model*num_round)"],"metadata":{"id":"yaR5GeBvT84c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(6):\n","  Batch_Size = Batch[i]\n","  print(\"\\nbatchsize : \",Batch_Size)\n","  for _ in range(num_run):\n","    runiidLSTM(hi_iid_datas,hi_iid_labels,test_data,test_label,Batch_Size)\n","    runiidGRU(hi_iid_datas,hi_iid_labels,test_data,test_label,Batch_Size)"],"metadata":{"id":"KKG3PJpVUB9x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## sort run"],"metadata":{"id":"EZcI4_xtYSC0"}},{"cell_type":"code","source":["# divide data\n","sort_direct(train_data, train_label)\n","hi_sort_datas, hi_sort_labels = split_equal_to_n(train_data, train_label, num_model*num_round)"],"metadata":{"id":"CCU420FtYSC1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i  in range(6):\n","  Batch_Size = Batch[i]\n","  print(\"\\nbatchsize : \",Batch_Size)\n","  for _ in range(num_run):\n","    runsortLSTM(hi_sort_datas,hi_sort_labels,test_data,test_label,Batch_Size)\n","    runsortGRU(hi_sort_datas,hi_sort_labels,test_data,test_label,Batch_Size)"],"metadata":{"id":"SUhyh6nyYSC1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## score run"],"metadata":{"id":"-bUGVDejaDxI"}},{"cell_type":"code","source":["hi_score_datas, hi_score_labels = split_label_score(train_data, train_label)\n","\n","data_12 = hi_score_datas[0]\n","data_12 = np.concatenate((data_12, hi_score_datas[1]),axis=0)\n","label_12 = hi_score_labels[0]\n","label_12 = np.concatenate((label_12, hi_score_labels[1]),axis=0)\n","\n","data_345 = hi_score_datas[2]\n","data_345 = np.concatenate((data_345, hi_score_datas[3]),axis=0)\n","data_345 = np.concatenate((data_345, hi_score_datas[4]),axis=0)\n","label_345 = hi_score_labels[2]\n","label_345 = np.concatenate((label_345, hi_score_labels[3]),axis=0)\n","label_345 = np.concatenate((label_345, hi_score_labels[4]),axis=0)\n","\n","shuffle_direct(data_12,label_12)\n","shuffle_direct(data_345,label_345)\n","\n","hi_12_datas, hi_12_labels = split_equal_to_n(data_12, label_12, num_round)\n","hi_345_datas, hi_345_labels = split_equal_to_n(data_345, label_345, num_round)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1700033351430,"user_tz":-480,"elapsed":53,"user":{"displayName":"專題羽球","userId":"16827757121216149927"}},"colab":{"base_uri":"https://localhost:8080/"},"id":"bNjd2k_YaDxJ","outputId":"178e8d4b-cfa9-4363-d3cf-a9073378a540"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-19-cfd2c46b98a0>:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  return np.array(client_data), np.array(client_label)\n"]}]},{"cell_type":"code","source":["for i in range(6):\n","  Batch_Size = Batch[i]\n","  print(\"\\nbatchsize : \",Batch_Size)\n","  for _ in range(num_run):\n","    runscoreLSTM(hi_12_datas,hi_12_labels,hi_345_datas,hi_345_labels,test_data,test_label,Batch_Size)\n","    runscoreGRU(hi_12_datas,hi_12_labels,hi_345_datas,hi_345_labels,test_data,test_label,Batch_Size)"],"metadata":{"id":"NGtBfPcWaDxJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# end"],"metadata":{"id":"ZC0QXP_oGBlN"}},{"cell_type":"code","source":[],"metadata":{"id":"iHovMipyYZg0"},"execution_count":null,"outputs":[]}]}