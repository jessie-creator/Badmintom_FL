{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Iq1usa3fsMnR","UVgdKmFTrb-i","BYX6K3HRrjeG","AApteznisFdi","zQQov0ZcshfA","crNSz29KGihe","ilFAL8-vG-8G","JFjhAarDHBfS","NmUVftq5u2Un","OeWWAS2grB8p","n-4de3cJrLqf","qeELg4N1sXIZ","GpwQ24mHyHNs","ajICESxkyKZt","m5RgRSmKs9Mx","3peGyCZnTWSG","iR1sjoWc9Gs_","kAplLa8lpS9c"],"authorship_tag":"ABX9TyMAHpc2jyWItxQBegh2ZqhN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VIzRifvMn44x"},"outputs":[],"source":["import numpy as np\n","import os\n","import csv\n","import random\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import LSTM, GRU, Dense, Input, Dropout\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n","from tensorflow.keras.optimizers import Adam\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["### constant"],"metadata":{"id":"Iq1usa3fsMnR"}},{"cell_type":"code","source":["# set manual seed for reproducibility\n","seed = 33\n","\n","# general reproducibility\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","#constant\n","num_lable_kinds = 5\n","both_cols = 6\n","high_rows = 60\n","short_rows = 50\n","\n","output_dim = 5"],"metadata":{"id":"UZnRekkYsMdf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["numpy to store data"],"metadata":{"id":"AHtwiF_jsTba"}},{"cell_type":"code","source":["#numpy to store data\n","high_data = np.empty((0, high_rows, both_cols), dtype=float)\n","high_label = np.empty((0, num_lable_kinds), dtype=int)\n","short_data = np.empty((0, short_rows, both_cols), dtype=float)\n","short_label = np.empty((0, num_lable_kinds), dtype=int)"],"metadata":{"id":"2iId00LOsY1m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# file process"],"metadata":{"id":"UVgdKmFTrb-i"}},{"cell_type":"markdown","source":["## funnction"],"metadata":{"id":"BYX6K3HRrjeG"}},{"cell_type":"markdown","source":["function of finding a hit"],"metadata":{"id":"u0uEMqOsrp9M"}},{"cell_type":"code","source":["def slope(input_data):\n","    slope_col = np.zeros(len(input_data),dtype=float)\n","    for i in range(len(input_data) - 1):\n","        for j in range(1,4):\n","            temp = input_data[i+1][j] - input_data[i][j]\n","            slope_col[i] += temp**2\n","        slope_col[i] = slope_col[i]**0.5\n","    return slope_col\n","\n","def ma(slope_col, n):\n","    slope_ma = np.zeros(len(slope_col),dtype=float)\n","    for i in range(n,len(slope_col)-n):\n","        for j in range(-n,n+1):\n","            slope_ma[i] += slope_col[i+j]\n","        slope_ma[i] /= float(2*n+1)\n","    return slope_ma\n","\n","def data_cut(input_data, save_data, hit_type):\n","    slope_col = slope(input_data)\n","    slope_avg = np.average(slope_col)\n","    # n = 4\n","    if hit_type == \"high\":\n","        n = 6\n","    elif hit_type == \"short\":\n","        n = 5\n","    slope_ma = ma(slope_col, n)\n","\n","    for i in range(60,len(slope_col)-50):\n","\n","        if ( slope_ma[i] > slope_avg ) and ( slope_col[i]==max(slope_col[i-50:i+50]) ) : # 找到可能峰值\n","            start = 0  # 向前&向後找起點\n","            end = 0\n","            while i+start > (50+n) :\n","                start -= 1\n","                if slope_ma[i+start] <= slope_avg:\n","                    break\n","            while i+end < (len(slope_col)-50-n) :\n","                end += 1\n","                if slope_ma[i+end] <= slope_avg:\n","                    break\n","\n","            if  hit_type == \"high\" and end-start > 40 and slope_ma[i] > 15: # 長遠球\n","                save_data.append(input_data[i-45:i+15, [1,2,3,5,6,7]])\n","\n","            elif hit_type == \"short\" and end-start > 20 and slope_ma[i] > 15: # 挑球\n","                save_data.append(input_data[i-35:i+15, [1,2,3,5,6,7]])"],"metadata":{"id":"rk15gCBnrf_e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["function to sanitize"],"metadata":{"id":"B8PCktUIrtvC"}},{"cell_type":"code","source":["# Removes null bytes from the input file and returns a sanitized version of the file.\n","def sanitize_file(input_file_path):\n","    sanitized_content = \"\"\n","    with open(input_file_path, 'r', encoding='utf-8', errors='replace') as f:\n","        content = f.read()\n","        sanitized_content = content.replace('\\x00', '')\n","\n","    return sanitized_content"],"metadata":{"id":"vK7-ZHnWrwp-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["function to transter file into numpy"],"metadata":{"id":"kWGkJBr3r9Eq"}},{"cell_type":"code","source":["def hit_preprocess(input_file_path, hit_type):\n","\n","  # Sanitize the file by removing null bytes\n","  sanitized_content = sanitize_file(input_file_path)\n","  lines = sanitized_content.split('\\n')\n","\n","  # Starting from the last line, move upwards until a complete line (with 7 commas) is found\n","  while lines and lines[-1].count(\",\") != 7:\n","    lines = lines[:-1]\n","\n","  # Load the (potentially modified) data into a numpy array\n","  input_data = np.loadtxt(lines, delimiter=\",\", dtype=float)\n","\n","  cut_data = []\n","  data_cut(input_data, cut_data, hit_type)\n","  cut_data = np.array(cut_data).astype(float)\n","\n","  datas_label = np.empty((0, num_lable_kinds), dtype=int)\n","  label_values = np.array([int(ch) for ch in input_filename[6:11]])  # 轉換為整數陣列\n","  label_values = label_values[np.newaxis, :]  # 增加一個維度以使其成為二維陣列\n","  for i in range(len(cut_data)):\n","    datas_label = np.vstack((datas_label, label_values))\n","\n","  return cut_data,datas_label"],"metadata":{"id":"UVqKqJ9zr9u6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## process"],"metadata":{"id":"AApteznisFdi"}},{"cell_type":"code","source":["# 指定資料集所在的路徑\n","data_path = \"/content/\"\n","# 取得該路徑下所有的檔案名稱\n","all_files = os.listdir(data_path)\n","# 過濾出所有的 .txt 檔案\n","txt_files = [file_name for file_name in all_files if file_name.endswith('.txt')]\n","\n","for input_filename in txt_files:\n","\n","  if \"high\" in input_filename.lower():\n","    hit_type = \"high\"\n","  elif \"short\" in input_filename.lower():\n","    hit_type = \"short\"\n","  else:\n","    print(\"error txt-file\")\n","    continue\n","\n","  input_file_path = os.path.join(data_path, input_filename)\n","  data, label = hit_preprocess(input_file_path, hit_type)\n","\n","  if hit_type == \"high\":\n","    high_data = np.concatenate((high_data, data), axis=0)\n","    high_label = np.concatenate((high_label, label))\n","  elif hit_type == \"short\":\n","    short_data = np.concatenate((short_data, data), axis=0)\n","    short_label = np.concatenate((short_label, label))\n","\n","  os.remove(input_file_path)"],"metadata":{"id":"flL-ZmWZsHVn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# reorder function"],"metadata":{"id":"zQQov0ZcshfA"}},{"cell_type":"markdown","source":["### shuffle, shuffle_direct"],"metadata":{"id":"crNSz29KGihe"}},{"cell_type":"code","source":["#input:data and label, output:numpy with same shape, but shuffled\n","#process: shuffle (data,label)\n","def shuffle(datas,labels):\n","  if len(datas) != len(labels):\n","    print(\"error\")\n","    return\n","\n","\n","  shuffled_datas = []\n","  shuffled_labels = []\n","\n","  # Shuffle the data indices\n","  indices = np.arange(len(datas))\n","  np.random.shuffle(indices)\n","\n","  for i in range(len(datas)):\n","    shuffled_datas.append(datas[indices[i]])\n","    shuffled_labels.append(labels[indices[i]])\n","\n","  return np.array(shuffled_datas), np.array(shuffled_labels)"],"metadata":{"id":"1CFXbX6eGhFi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#direct change input's data and label\n","def shuffle_direct(datas,labels):\n","  if len(datas) != len(labels):\n","    print(\"error\")\n","    return\n","\n","\n","  shuffled_datas_list = []\n","  shuffled_labels_list = []\n","\n","  # Shuffle the data indices\n","  indices = np.arange(len(datas))\n","  np.random.shuffle(indices)\n","\n","  for i in range(len(datas)):\n","    shuffled_datas_list.append(datas[indices[i]])\n","    shuffled_labels_list.append(labels[indices[i]])\n","\n","  shuffled_datas = np.array(shuffled_datas_list)\n","  shuffled_labels = np.array(shuffled_labels_list)\n","\n","  datas[:] = shuffled_datas\n","  labels[:] = shuffled_labels"],"metadata":{"id":"GVtdS9jHGnEy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### sort, sort_direct"],"metadata":{"id":"ilFAL8-vG-8G"}},{"cell_type":"code","source":["#input:data and label, output:numpy with same shape, but sorted\n","#process: sort (data,label)\n","def sort(datas,labels):\n","  if len(datas) != len(labels):\n","    print(\"error\")\n","    return\n","\n","  label_sums = labels.sum(axis=1)\n","  sorted_indice = np.argsort(label_sums,kind='stable')\n","\n","  sorted_datas = []\n","  sorted_labels = []\n","\n","  for i in range(len(datas)):\n","    sorted_datas.append(datas[sorted_indice[i]])\n","    sorted_labels.append(labels[sorted_indice[i]])\n","\n","  return np.array(sorted_datas), np.array(sorted_labels)"],"metadata":{"id":"XlspZ7cUGpC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#direct change input's data and label\n","def sort_direct(datas,labels):\n","  if len(datas) != len(labels):\n","    print(\"error\")\n","    return\n","\n","  label_sums = labels.sum(axis=1)\n","  sorted_indice = np.argsort(label_sums,kind='stable')\n","\n","  sorted_datas_list = []\n","  sorted_labels_list = []\n","\n","  for i in range(len(datas)):\n","    sorted_datas_list.append(datas[sorted_indice[i]])\n","    sorted_labels_list.append(labels[sorted_indice[i]])\n","\n","  sorted_datas = np.array(sorted_datas_list)\n","  sorted_labels = np.array(sorted_labels_list)\n","\n","  datas[:] = sorted_datas\n","  labels[:] = sorted_labels"],"metadata":{"id":"tvdxpUZCGtUD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### split_percentage, split_equal_to_n"],"metadata":{"id":"JFjhAarDHBfS"}},{"cell_type":"code","source":["#input: data, label and percentage of test\n","#output: numpy, (train_data, train_label, test_data, test_label)\n","def split_percentage(datas,labels,test_percent):\n","  num_train = round(len(datas) * (1 - test_percent))\n","\n","  train_data = datas[:num_train]\n","  train_label = labels[:num_train]\n","  test_data = datas[num_train:]\n","  test_label = labels[num_train:]\n","  return train_data, train_label, test_data, test_label"],"metadata":{"id":"z6Dbbk-7HDnE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#input: data, label and number of client\n","#output: numpy, [ data1 data2 ...], data1 = output[0]\n","def split_equal_to_n(datas,labels,n_Clients):\n","  num_items_per_client = len(datas) // n_Clients\n","  client_data = []\n","  client_label = []\n","\n","  for i in range(n_Clients):\n","    start_idx = i * num_items_per_client\n","    end_idx = (i + 1) * num_items_per_client\n","    client_data.append(datas[start_idx:end_idx])\n","    client_label.append(labels[start_idx:end_idx])\n","\n","  return np.array(client_data), np.array(client_label)"],"metadata":{"id":"0bDMkr-KHGFe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### noniid_split"],"metadata":{"id":"NmUVftq5u2Un"}},{"cell_type":"code","source":["#input: data, label and percentage to split score12 and score345\n","#output: numpy, (dataone, labelone, datatwo, labeltwo)\n","#dataone: (1-percent)*score12 + percent*score345\n","def noniid_split(datas,labels,percent):\n","  if len(datas) != len(labels):\n","    print(\"error\")\n","    return\n","\n","  stdt, stlb = sort(datas,labels)\n","  means = np.mean(stlb, axis=1)\n","  rounded_means = np.round(means)\n","\n","  start_idx = 0\n","  end_idx = 0\n","\n","  for i in range(len(stlb)):\n","    if rounded_means[i] == 3:\n","      end_idx = i\n","      data12 = stdt[start_idx:end_idx]\n","      label12 = stlb[start_idx:end_idx]\n","      start_idx = i\n","      break\n","\n","  end_idx = len(stlb)\n","  data345 = stdt[start_idx:end_idx]\n","  label345 = stlb[start_idx:end_idx]\n","\n","  shuffle_direct(data12,label12)\n","  shuffle_direct(data345,label345)\n","\n","  data12_08, label12_08, data12_02, label12_02 = split_percentage(data12,label12,percent)\n","  data345_08, label345_08, data345_02, label345_02 = split_percentage(data345,label345,percent)\n","\n","  dataone = np.concatenate((data12_08, data345_02), axis=0)\n","  labelone = np.concatenate((label12_08,label345_02), axis=0)\n","\n","  datatwo = np.concatenate((data345_08,data12_02), axis=0)\n","  labeltwo = np.concatenate((label345_08,label12_02), axis=0)\n","\n","  shuffle_direct(dataone,labelone)\n","  shuffle_direct(datatwo,labeltwo)\n","\n","  return dataone, labelone, datatwo, labeltwo"],"metadata":{"id":"YJy9TiAGu2zY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# models"],"metadata":{"id":"OeWWAS2grB8p"}},{"cell_type":"markdown","source":["### output,loss,accuracy,CNNpreprocess"],"metadata":{"id":"n-4de3cJrLqf"}},{"cell_type":"code","source":["# Define custom output function\n","def custom_output(x):\n","  # Implement your custom output logic here\n","  return x\n","\n","def cus_loss(y_true, y_pred):\n","  y_true = tf.cast(y_true, dtype=tf.float32)  # Cast y_true to float32\n","  loss = tf.reduce_mean(tf.square(y_true - y_pred))\n","  return loss\n","\n","# Define custom accuracy function\n","threshold = 0.5\n","def cus_acc(y_true, y_pred):\n","  abs_diff = tf.abs(y_true - y_pred)\n","  condition = tf.less_equal(abs_diff, threshold)\n","  acc = tf.cast(condition, tf.float32)\n","  return acc\n","\n","# 為了適應CNN的輸入要求，在數據集加載過程中添加一個維度\n","def CNNpreprocess(data):\n","  # 增加一個維度以模擬“通道”\n","  return tf.expand_dims(data, -1)"],"metadata":{"id":"45wpIrIBrF7Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### LSTM_mymodel"],"metadata":{"id":"qeELg4N1sXIZ"}},{"cell_type":"code","source":["class LSTM_mymodel(Model):\n","  def __init__(self, units, output_dim, num_layers, dropout_rate):\n","    super(LSTM_mymodel, self).__init__()\n","    self.lstm_layers = [LSTM(units, return_sequences=(i < num_layers - 1)) for i in range(num_layers)]\n","    self.dropout = Dropout(rate=dropout_rate)\n","    self.dense = Dense(output_dim)\n","\n","  def call(self, inputs):\n","    x = inputs\n","    for lstm_layer in self.lstm_layers:\n","      x = lstm_layer(x)\n","      x = self.dropout(x)  # Apply dropout after each LSTM layer\n","    output = self.dense(x)\n","    output = custom_output(output)\n","    return output"],"metadata":{"id":"ITNIR39EsZKI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GRU_mymodel"],"metadata":{"id":"GpwQ24mHyHNs"}},{"cell_type":"code","source":["class GRU_mymodel(Model):\n","  def __init__(self, units, output_dim, num_layers, dropout_rate):\n","    super(GRU_mymodel, self).__init__()\n","    self.gru_layers = [GRU(units, return_sequences=(i < num_layers - 1)) for i in range(num_layers)]\n","    self.dropout = Dropout(rate=dropout_rate)\n","    self.dense = Dense(output_dim)\n","\n","  def call(self, inputs):\n","    x = inputs\n","    for gru_layer in self.gru_layers:\n","      x = gru_layer(x)\n","      x = self.dropout(x)  # Apply dropout after each GRU layer\n","    output = self.dense(x)\n","    output = custom_output(output)\n","    return output"],"metadata":{"id":"y2F0-0BpyIkw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### CNN_mymodel"],"metadata":{"id":"ajICESxkyKZt"}},{"cell_type":"code","source":["# CNN\n","class CNN_mymodel(Model):\n","  def __init__(self, output_dim, num_filters, kernel_size, dropout_rate):\n","    super(CNN_mymodel, self).__init__()\n","    self.conv1 = Conv2D(num_filters, kernel_size, activation='relu', padding='same')\n","    self.pool1 = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')\n","    self.conv2 = Conv2D(num_filters, kernel_size, activation='relu', padding='same')\n","    self.pool2 = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')\n","    self.flatten = Flatten()\n","    self.dropout = Dropout(rate=dropout_rate)\n","    self.dense = Dense(output_dim)\n","\n","  def call(self, inputs):\n","    x = self.conv1(inputs)\n","    x = self.pool1(x)\n","    x = self.conv2(x)\n","    x = self.pool2(x)\n","    x = self.flatten(x)\n","    x = self.dropout(x)\n","    output = self.dense(x)\n","    output = custom_output(output)\n","    return output"],"metadata":{"id":"AR4vqxiJyLf1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### function to average weight"],"metadata":{"id":"m5RgRSmKs9Mx"}},{"cell_type":"code","source":["def average_weight(models):\n","  #avg\n","  avg_weights = list()\n","  nummodel = len(models)\n","  for i,model in enumerate(models):\n","    weights = model.get_weights()\n","\n","    for j in range(len(weights)):\n","      if i==0:\n","        avg_weights.append(weights[j])\n","      else:\n","        avg_weights[j]=avg_weights[j]+weights[j]\n","\n","  for i in range(len(weights)):\n","    avg_weights[i]=avg_weights[i] / nummodel\n","\n","  # set\n","  for i,model in enumerate(models):\n","    weights = model.get_weights()\n","\n","    for j in range(len(weights)):\n","      weights[j] = avg_weights[j]\n","\n","    model.set_weights(weights)"],"metadata":{"id":"14HIeMvQs8wo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### to_tfdata"],"metadata":{"id":"3peGyCZnTWSG"}},{"cell_type":"code","source":["def to_tfdata(datas,labels):\n","  tf_datas = []\n","  datalists = [datas[i] for i in range(datas.shape[0])]\n","  labellists = [labels[i] for i in range(labels.shape[0])]\n","  for i in range(len(datalists)):\n","    temp_tfdata = tf.data.Dataset.from_tensor_slices((datalists[i], labellists[i]))\n","    temp_tfdata = temp_tfdata.map(lambda x, y: (CNNpreprocess(x), y))\n","    tf_datas.append(temp_tfdata)\n","  return tf_datas"],"metadata":{"id":"f8a9WmjzTZlz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Main"],"metadata":{"id":"5S7kF7BTOgEq"}},{"cell_type":"markdown","source":["### constant"],"metadata":{"id":"yOMJ42Rtzo2H"}},{"cell_type":"code","source":["test_percent = 0.1\n","\n","Units=64\n","Output_Dim = 5\n","Num_Layers = 1\n","Num_Filters = 64\n","Kernel_Size = (7, 7)\n","\n","#learning_rate\n","LSTM_lr = 0.01\n","GRU_lr = 0.01\n","CNN_lr = 0.0001\n","\n","#dropout\n","LSTM_drop = 0\n","GRU_drop = 0\n","CNN_drop = 0\n","Dp_Rates = [0.0,0.2]\n","\n","#batchsize\n","LSTM_bs = 20\n","GRU_bs = 20\n","CNN_bs = 20\n","\n","#Epoch\n","Epoch = 50\n","CNN_Epoch = 30\n","\n","# constant for fedavg\n","num_model = 3\n","num_round = 2\n","num_fit = 10\n","# constant"],"metadata":{"id":"ktbfVTNLxlRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(high_data.shape)\n","shuffle_direct(high_data, high_label)\n","train_data, train_label, test_data, test_label = split_percentage(high_data,high_label,test_percent)\n","print(train_data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A0Mi6ktZwPqF","executionInfo":{"status":"ok","timestamp":1701584410861,"user_tz":-480,"elapsed":8,"user":{"displayName":"專題羽球","userId":"16827757121216149927"}},"outputId":"0ff821fb-8144-4bff-c9df-62674fdd76a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(848, 60, 6)\n","(763, 60, 6)\n"]}]},{"cell_type":"markdown","source":["### plot function"],"metadata":{"id":"iR1sjoWc9Gs_"}},{"cell_type":"code","source":["def plot_drop(num_ep,dt_train,dt_val,title):\n","  epochs = range(1,num_ep+1)\n","  plt.plot(epochs, dt_train, 'g', label='train')\n","  plt.plot(epochs, dt_val, 'b', label='validation')\n","  plt.title(title)\n","  plt.xlabel('Epochs')\n","  plt.legend()\n","  plt.show()\n","  print(\"Epoch\",num_ep,\": \",dt_val[num_ep-1])"],"metadata":{"id":"0r_zFCzDFJsE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## centralize"],"metadata":{"id":"dTFTYqQMQk3o"}},{"cell_type":"code","source":["shuffle_direct(high_data, high_label)\n","train_data, train_label, test_data, test_label = split_percentage(high_data,high_label,test_percent)"],"metadata":{"id":"n8o7HZdpQoxV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cen_LSTM = LSTM_mymodel(units=Units,output_dim=Output_Dim,num_layers=Num_Layers,dropout_rate=LSTM_drop)\n","cen_LSTM.compile(optimizer=Adam(learning_rate=LSTM_lr),loss=cus_loss, metrics=[cus_acc])\n","cen_LSTM_history = cen_LSTM.fit(train_data,train_label,epochs=Epoch,batch_size=LSTM_bs,\n","                                validation_data=(test_data, test_label))"],"metadata":{"id":"EfeG3U_SQqcs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701584433476,"user_tz":-480,"elapsed":22620,"user":{"displayName":"專題羽球","userId":"16827757121216149927"}},"outputId":"e0a55def-413d-4a53-f556-42e90d4d140c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","39/39 [==============================] - 6s 98ms/step - loss: 2.6196 - cus_acc: 0.2391 - val_loss: 1.4722 - val_cus_acc: 0.3035\n","Epoch 2/10\n","39/39 [==============================] - 3s 65ms/step - loss: 1.3072 - cus_acc: 0.3578 - val_loss: 1.2901 - val_cus_acc: 0.3412\n","Epoch 3/10\n","39/39 [==============================] - 2s 57ms/step - loss: 1.0503 - cus_acc: 0.4296 - val_loss: 0.9676 - val_cus_acc: 0.4094\n","Epoch 4/10\n","39/39 [==============================] - 1s 29ms/step - loss: 0.8096 - cus_acc: 0.4813 - val_loss: 0.8185 - val_cus_acc: 0.4565\n","Epoch 5/10\n","39/39 [==============================] - 1s 28ms/step - loss: 0.7996 - cus_acc: 0.4792 - val_loss: 1.0958 - val_cus_acc: 0.4494\n","Epoch 6/10\n","39/39 [==============================] - 1s 30ms/step - loss: 0.5957 - cus_acc: 0.5486 - val_loss: 0.9084 - val_cus_acc: 0.4776\n","Epoch 7/10\n","39/39 [==============================] - 1s 29ms/step - loss: 0.5591 - cus_acc: 0.5704 - val_loss: 0.6009 - val_cus_acc: 0.5247\n","Epoch 8/10\n","39/39 [==============================] - 1s 29ms/step - loss: 0.5540 - cus_acc: 0.5628 - val_loss: 0.6589 - val_cus_acc: 0.5224\n","Epoch 9/10\n","39/39 [==============================] - 1s 36ms/step - loss: 0.4542 - cus_acc: 0.5955 - val_loss: 0.4703 - val_cus_acc: 0.5718\n","Epoch 10/10\n","39/39 [==============================] - 2s 45ms/step - loss: 0.3828 - cus_acc: 0.6346 - val_loss: 0.5126 - val_cus_acc: 0.5553\n"]}]},{"cell_type":"code","source":["print(cen_LSTM_history.history.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jRXr9DrKQs-9","executionInfo":{"status":"ok","timestamp":1701584433476,"user_tz":-480,"elapsed":6,"user":{"displayName":"專題羽球","userId":"16827757121216149927"}},"outputId":"c435c723-2acf-4fa6-9ce8-d4f6dceee64e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['loss', 'cus_acc', 'val_loss', 'val_cus_acc'])\n"]}]},{"cell_type":"markdown","source":["## IID"],"metadata":{"id":"m4m-YNHARC0D"}},{"cell_type":"code","source":["shuffle_direct(high_data, high_label)\n","train_data, train_label, test_data, test_label = split_percentage(high_data,high_label,test_percent)\n","iid_datas, iid_labels = split_equal_to_n(train_data, train_label, num_model*num_round)\n","\n","iid_Epoch = Epoch*num_round//num_fit"],"metadata":{"id":"pqB8KV7HRCGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for drop in Dp_Rates:\n","  iid_LSTM_models = []\n","  loss = []\n","  val_loss = []\n","  cus_acc = []\n","  val_cus_acc = []\n","\n","  for i in range(num_model):\n","    iid_LSTM_models.append(LSTM_mymodel(units=Units,output_dim=Output_Dim,num_layers=Num_Layers,dropout_rate=LSTM_drop))\n","  for model in iid_LSTM_models:\n","    model.compile(optimizer=Adam(learning_rate=LSTM_lr),loss=cus_loss, metrics=[cus_acc])\n","  # average_weight(models)\n","\n","  for j in range(num_fit):\n","    for i, model in enumerate(iid_LSTM_models):\n","      temp = j*num_round//num_fit\n","      iid_LSTM_history = model.fit(iid_datas[temp*num_model+i],iid_labels[temp*num_model+i],epochs=Epoch,\n","                          batch_size=LSTM_bs,verbose=0,validation_data=(test_data,test_label))\n","      print(iid_LSTM_history.history.keys())"],"metadata":{"id":"COjvpHCkRZTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["iid_LSTM_models = []\n","iid_LSTM_loss = []\n","iid_LSTM_val_loss = []\n","iid_LSTM_cus_acc = []\n","iid_LSTM_val_cus_acc = []\n","\n","for i in range(num_model):\n","  iid_LSTM_models.append(LSTM_mymodel(units=Units, output_dim=Output_Dim, num_layers=Num_Layers,dropout_rate=LSTM_drop))\n","\n","for model in iid_LSTM_models:\n","  model.compile(optimizer=Adam(learning_rate=LSTM_lr), loss=cus_loss, metrics=[cus_acc])\n","\n","for j in range(num_fit):\n","  for i, model in enumerate(iid_LSTM_models):\n","    temp = j*num_round//num_fit\n","    iidhistory=model.fit(iid_datas[temp*num_model+i],iid_labels[temp*num_model+i],epochs=iid_Epoch,batch_size=LSTM_bs,\n","                         verbose=0,validation_data=(test_data,test_label))\n","    print(iidhistory.history.keys())\n","    if j == 0 :\n","      iid_LSTM_loss.append(iidhistory.history['loss'])\n","      iid_LSTM_val_loss.append(iidhistory.history['val_loss'])\n","      iid_LSTM_cus_acc.append(iidhistory.history['cus_acc'])\n","      iid_LSTM_val_cus_acc.append(iidhistory.history['val_cus_acc'])\n","    else:\n","      iid_LSTM_loss[i].extend(iidhistory.history['loss'])\n","      iid_LSTM_val_loss[i].extend(iidhistory.history['val_loss'])\n","      iid_LSTM_cus_acc[i].extend(iidhistory.history['cus_acc'])\n","      iid_LSTM_val_cus_acc[i].extend(iidhistory.history['val_cus_acc'])\n","  avg_weight = average_weight(iid_LSTM_models)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uC42OE3FWqUO","executionInfo":{"status":"error","timestamp":1701584806371,"user_tz":-480,"elapsed":7719,"user":{"displayName":"專題羽球","userId":"16827757121216149927"}},"outputId":"17569196-73a6-4f07-810a-7e5bd257900a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['loss', 'val_loss'])\n"]},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-131-ac913660f142>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0miid_LSTM_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miidhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0miid_LSTM_val_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miidhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0miid_LSTM_cus_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miidhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cus_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0miid_LSTM_val_cus_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miidhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_cus_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'cus_acc'"]}]},{"cell_type":"markdown","source":["# end"],"metadata":{"id":"kAplLa8lpS9c"}},{"cell_type":"code","source":[],"metadata":{"id":"5K7V98OopUBJ"},"execution_count":null,"outputs":[]}]}